<!DOCTYPE html>
<html lang="en">

<head>
    
    
    
    <!-- Non social metatags -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    
    
    <title>决策树ID3原理及R语言python代码实现</title>
    
    
    












<!-- Place this data between the <head> tags of your website -->

<meta name="description" content="决策树是机器学习中一种非常常见的分类与回归方法，可以认为是if-else结构的规则。分类决策树是由节点和有向边组成的树形结构，节点表示特征或者属性， 而边表示的是属性值，边指向的叶节点为对应的分类。在对样本的分类过程中，由顶向下，根据特征或属性值选择分支，递归遍历直到叶节点，将实例分到叶节点对应的类别中。 决策树的学习过程就是构造出一个能正取分类（或者误差最小）训练数据集的且有较好泛化能力的树，核心是如何选择特征或属性作为节点， 通常的算法是利用启发式的算法如ID3，C4.5，CART等递归的选择最优特征。选择一个最优特征，然后按照此特征将数据集分割成多个子集，子集再选择最优特征， 直到..." />

  <meta name="keywords" itemprop="tags" content="python, 机器学习, 决策树ID3"/>



  <meta name="keywords" itemprop="categories" content="ai" />



<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image" />



<meta name="twitter:title" content="决策树ID3原理及R语言python代码实现" />
<meta name="twitter:description" content="决策树是机器学习中一种非常常见的分类与回归方法，可以认为是if-else结构的规则。分类决策树是由节点和有向边组成的树形结构，节点表示特征或者属性， 而边表示的是属性值，边指向的叶节点为对应的分类。在对样本的分类过程中，由顶向下，根据特征或属性值选择分支，递归遍历直到叶节点，将实例分到叶节点对应的类别中。 决策树的学习过程就是构造出一个能正取分类（或者误差最小）训练数据集的且有较好泛化能力的树，核心是如何选择特征或属性作为节点， 通常的算法是利用启发式的算法如ID3，C4.5，CART等递归的选择最优特征。选择一个最优特征，然后按照此特征将数据集分割成多个子集，子集再选择最优特征， 直到..." />


  <meta name="twitter:creator" content="@evanown" />


<!-- Twitter summary card with large image must be at least 280x150px -->

  <meta name="twitter:image:src" content="https://h2cloud.org/thumbnail-jumbo.png" />
  <meta name="twitter:image" content="https://h2cloud.org/thumbnail-jumbo.png" />

<meta name="twitter:url" content="https://h2cloud.org//ai/jcsid3.html" />

<!-- Open Graph data -->
<meta property="og:title" content="决策树ID3原理及R语言python代码实现" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://h2cloud.org//ai/jcsid3.html" />


  <meta property="og:image" content="https://h2cloud.org/thumbnail-jumbo.png" />

<meta property="og:description" content="决策树是机器学习中一种非常常见的分类与回归方法，可以认为是if-else结构的规则。分类决策树是由节点和有向边组成的树形结构，节点表示特征或者属性， 而边表示的是属性值，边指向的叶节点为对应的分类。在对样本的分类过程中，由顶向下，根据特征或属性值选择分支，递归遍历直到叶节点，将实例分到叶节点对应的类别中。 决策树的学习过程就是构造出一个能正取分类（或者误差最小）训练数据集的且有较好泛化能力的树，核心是如何选择特征或属性作为节点， 通常的算法是利用启发式的算法如ID3，C4.5，CART等递归的选择最优特征。选择一个最优特征，然后按照此特征将数据集分割成多个子集，子集再选择最优特征， 直到..." />
<meta property="og:site_name" content="h2cloud" />


<meta property="og:locale" content="" />


  <meta property="article:published_time" content="2019-08-23T00:00:00+08:00" />




  
    <meta property="article:tag" content="python" />
  
    <meta property="article:tag" content="机器学习" />
  
    <meta property="article:tag" content="决策树ID3" />
  





  
    <meta property="article:tag" content="ai" />
  




    
    
    <link rel="canonical" href="https://h2cloud.org/ai/jcsid3.html">
    
    
    
    <link rel="shortcut icon" href="https://h2cloud.org/favicon.ico">
    
    <meta name="robots" content="noarchive">
    
    <!-- <link rel="alternate" media="only screen and (max-width: 640px)" href="">
        <link rel="alternate" media="handheld" href=""> -->
        
        
        <link rel="stylesheet" href="https://h2cloud.org/assets/css/style.css?v=">
    </head>
    <body>
        
        <header class="site-header" role="banner">

  <div class="wrapper">
    
    

    
      <a class="site-title" href="https://h2cloud.org/">h2cloud</a>
    

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/h2engine.html">H2engine</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/fflib.html">FFlib</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/fflua.html">FFlua</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/ffpython.html">FFpython</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/gamedev.html">Gamedev</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/ai.html">AI</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/tech.html">Tech</a>
              
            
          
            
            
              
                <a class="page-link" href="https://h2cloud.org/about.html">About</a>
              
            
          
        </div>
      </nav>
    
  </div>
</header>

        
        
        
        
        
        <section class="page-header">
            <h1 class="project-name">决策树ID3原理及R语言python代码实现</h1>
            <h2 class="project-tagline">决策树是机器学习中一种非常常见的分类与回归方法，可以认为是if-else结构的规则。</h2>
            
            <!-- Post tagline -->
            
            <h2 class="project-date">
                <time datetime="2019-08-23T00:00:00+08:00" itemprop="datePublished">
                    
                    Aug 23, 2019
                </time>
                
                
                • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Evan Zhao</span></span>
                
            </h2>
            
            <!-- End: Post tagline -->
        </section>
        
        <section class="main-content">
            
            <article itemscope itemtype="http://schema.org/BlogPosting">

  <!-- <header class="post-header">
    <h1 class="post-title" itemprop="name headline">决策树ID3原理及R语言python代码实现</h1>
    <p class="post-meta">
      <time datetime="2019-08-23T00:00:00+08:00" itemprop="datePublished">
        
        Aug 23, 2019
      </time>
      </p>
  </header> -->

  <div itemprop="articleBody">
    <h1 id="决策树id3原理及r语言python代码实现西瓜书">决策树ID3原理及R语言python代码实现（西瓜书）</h1>
<h2 id="摘要">摘要：</h2>
<p>决策树是机器学习中一种非常常见的分类与回归方法，可以认为是if-else结构的规则。分类决策树是由节点和有向边组成的树形结构，节点表示特征或者属性，
而边表示的是属性值，边指向的叶节点为对应的分类。在对样本的分类过程中，由顶向下，根据特征或属性值选择分支，递归遍历直到叶节点，将实例分到叶节点对应的类别中。
决策树的学习过程就是构造出一个能正取分类（或者误差最小）训练数据集的且有较好泛化能力的树，核心是如何选择特征或属性作为节点，
通常的算法是利用启发式的算法如ID3，C4.5，CART等递归的选择最优特征。选择一个最优特征，然后按照此特征将数据集分割成多个子集，子集再选择最优特征，
直到所有训练数据都被正取分类，这就构造出了决策树。决策树有如下特点：</p>
<ol>
  <li>原理简单, 计算高效；使用基于信息熵相关的理论划分最优特征，原理清晰，计算效率高。</li>
  <li>解释性强；决策树的属性结构以及if-else的判断逻辑，非常符合人的决策思维，使用训练数据集构造出一个决策树后，可视化决策树，
可以非常直观的理解决策树的判断逻辑，可读性强。</li>
  <li>效果好，应用广泛;其拟合效果一般很好，分类速度快，但也容易过拟合，决策树拥有非常广泛的应用。</li>
</ol>

<p>本文主要介绍基于ID3的算法构造决策树。</p>

<h2 id="决策树原理">决策树原理</h2>
<p>训练数据集有多个特征，如何递归选择最优特征呢？信息熵增益提供了一个非常好的也非常符合人们日常逻辑的判断准则，即信息熵增益最大的特征为最优特征。在信息论中，熵是用来度量随机变量不确定性的量纲，熵越大，不确定性越大。熵定义如下:</p>

<p><img src="/assets/img/jcs/jcs1.png" alt=""></p>

<p>此处log一般是以2为底，假设一个产品成品率为100%次品率为0%那么熵就为0，如果是成品率次品率各为50%，那么熵就为1，熵越大，说明不确定性越高，非常符合我们人类的思维逻辑。假设分类标记为随机变量Y，那么H(Y)表示随机变量Y的不确定性，我们依次选择可选特征，如果选择一个特征后，随机变量Y的熵减少的最多，表示得知特征X后，使得类Y不确定性减少最多，那么就把此特征选为最优特征。信息熵增益的公式如下：</p>

<p><img src="/assets/img/jcs/jcs2.png" alt=""></p>

<h2 id="id3算法">ID3算法</h2>

<p>决策树基于信息熵增益的ID3算法步骤如下：</p>

<ol>
  <li>如果数据集类别只有一类，选择这个类别作为，标记为叶节点。</li>
  <li>从数据集的所有特征中，选择信息熵增益最大的作为节点，特征的属性分别作为节点的边。</li>
  <li>选择最优特征后，按照对应的属性，将数据集分成多个，依次将子数据集从第1步递归进行构造子树。</li>
</ol>

<h2 id="python实现">python实现</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#encoding:utf-8
</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span>  <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">DecisionTree</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">def</span> <span class="nf">calEntropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1"># 计算熵
</span>        <span class="n">valRate</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="c1"># 频次汇总 得到各个特征对应的概率
</span>        <span class="n">valEntropy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="n">valRate</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">valRate</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">valEntropy</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">yTrain</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#如果不传，自动选择最后一列作为分类标签
</span>            <span class="n">yTrain</span> <span class="o">=</span> <span class="n">xTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">xTrain</span> <span class="o">=</span> <span class="n">xTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="nb">len</span><span class="p">(</span><span class="n">xTrain</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buildDecisionTree</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
    <span class="k">def</span> <span class="nf">buildDecisionTree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">):</span>
        <span class="n">propNamesAll</span> <span class="o">=</span> <span class="n">xTrain</span><span class="o">.</span><span class="n">columns</span>
        <span class="c1">#print(propNamesAll)
</span>        <span class="n">yTrainCounts</span> <span class="o">=</span> <span class="n">yTrain</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">yTrainCounts</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1">#print('only one class', yTrainCounts.index[0])
</span>            <span class="k">return</span> <span class="n">yTrainCounts</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">entropyD</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calEntropy</span><span class="p">(</span><span class="n">yTrain</span><span class="p">)</span>

        <span class="n">maxGain</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">maxEntropyPropName</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">propName</span> <span class="ow">in</span> <span class="n">propNamesAll</span><span class="p">:</span>
            <span class="n">propDatas</span> <span class="o">=</span> <span class="n">xTrain</span><span class="p">[</span><span class="n">propName</span><span class="p">]</span>
            <span class="n">propClassSummary</span> <span class="o">=</span> <span class="n">propDatas</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">/</span> <span class="n">propDatas</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="c1"># 频次汇总 得到各个特征对应的概率
</span>
            <span class="n">sumEntropyByProp</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">propClass</span><span class="p">,</span> <span class="n">dvRate</span> <span class="ow">in</span> <span class="n">propClassSummary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">yDataByPropClass</span> <span class="o">=</span> <span class="n">yTrain</span><span class="p">[</span><span class="n">xTrain</span><span class="p">[</span><span class="n">propName</span><span class="p">]</span> <span class="o">==</span> <span class="n">propClass</span><span class="p">]</span>
                <span class="n">entropyDv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calEntropy</span><span class="p">(</span><span class="n">yDataByPropClass</span><span class="p">)</span>
                <span class="n">sumEntropyByProp</span> <span class="o">+=</span> <span class="n">entropyDv</span> <span class="o">*</span> <span class="n">dvRate</span>
            <span class="n">gainEach</span> <span class="o">=</span> <span class="n">entropyD</span> <span class="o">-</span> <span class="n">sumEntropyByProp</span>
            <span class="k">if</span> <span class="n">maxGain</span> <span class="o">==</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">gainEach</span> <span class="o">&gt;</span> <span class="n">maxGain</span><span class="p">:</span>
                <span class="n">maxGain</span> <span class="o">=</span> <span class="n">gainEach</span>
                <span class="n">maxEntropyPropName</span> <span class="o">=</span> <span class="n">propName</span>
        <span class="c1">#print('select prop:', maxEntropyPropName, maxGain)
</span>        <span class="n">propDatas</span> <span class="o">=</span> <span class="n">xTrain</span><span class="p">[</span><span class="n">maxEntropyPropName</span><span class="p">]</span>
        <span class="n">propClassSummary</span> <span class="o">=</span> <span class="n">propDatas</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">/</span> <span class="n">propDatas</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="c1"># 频次汇总 得到各个特征对应的概率
</span>        
        <span class="n">retClassByProp</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">propClass</span><span class="p">,</span> <span class="n">dvRate</span> <span class="ow">in</span> <span class="n">propClassSummary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">whichIndex</span> <span class="o">=</span> <span class="n">xTrain</span><span class="p">[</span><span class="n">maxEntropyPropName</span><span class="p">]</span> <span class="o">==</span> <span class="n">propClass</span>
            <span class="k">if</span> <span class="n">whichIndex</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">xDataByPropClass</span> <span class="o">=</span> <span class="n">xTrain</span><span class="p">[</span><span class="n">whichIndex</span><span class="p">]</span>
            <span class="n">yDataByPropClass</span> <span class="o">=</span> <span class="n">yTrain</span><span class="p">[</span><span class="n">whichIndex</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">xDataByPropClass</span><span class="p">[</span><span class="n">maxEntropyPropName</span><span class="p">]</span><span class="c1">#删除已经选择的属性列
</span>            
            <span class="c1">#print(propClass)
</span>            <span class="c1">#print(pd.concat([xDataByPropClass, yDataByPropClass], axis=1))
</span>            
            <span class="n">retClassByProp</span><span class="p">[</span><span class="n">propClass</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buildDecisionTree</span><span class="p">(</span><span class="n">xDataByPropClass</span><span class="p">,</span> <span class="n">yDataByPropClass</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span><span class="s">'Node'</span><span class="p">:</span><span class="n">maxEntropyPropName</span><span class="p">,</span> <span class="s">'Edge'</span><span class="p">:</span><span class="n">retClassByProp</span><span class="p">}</span>
    <span class="k">def</span> <span class="nf">predictBySeries</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modelNode</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">modelNode</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">modelNode</span>
        <span class="n">nodePropName</span> <span class="o">=</span> <span class="n">modelNode</span><span class="p">[</span><span class="s">'Node'</span><span class="p">]</span>
        <span class="n">prpVal</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">nodePropName</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">edge</span><span class="p">,</span> <span class="n">nextNode</span> <span class="ow">in</span> <span class="n">modelNode</span><span class="p">[</span><span class="s">'Edge'</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">prpVal</span> <span class="o">==</span> <span class="n">edge</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictBySeries</span><span class="p">(</span><span class="n">nextNode</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictBySeries</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictBySeries</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">dataTrain</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"xiguadata.csv"</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">"gbk"</span><span class="p">)</span>

<span class="n">decisionTree</span> <span class="o">=</span> <span class="n">DecisionTree</span><span class="p">()</span>
<span class="n">treeData</span> <span class="o">=</span> <span class="n">decisionTree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'预测值'</span><span class="p">:</span><span class="n">decisionTree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">),</span> <span class="s">'正取值'</span><span class="p">:</span><span class="n">dataTrain</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]}))</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="k">print</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">treeData</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>

</code></pre></div></div>

<p>训练结束后，使用一个递归的字典保存决策树模型，使用格式json工具格式化输出后，可以简洁的看到树的结构。</p>

<p><img src="/assets/img/jcs/jcs3.jpg" alt=""></p>

<h2 id="r语言实现">R语言实现</h2>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">

</span><span class="n">dataTrain</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"xiguadata.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="n">trainDecisionTree</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">){</span><span class="w">
    </span><span class="n">calEntropy</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">y</span><span class="p">){</span><span class="w"> </span><span class="c1"># 计算熵</span><span class="w">

        </span><span class="n">values</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">y</span><span class="p">));</span><span class="w"> </span><span class="c1"># 频次汇总 得到各个特征对应的概率</span><span class="w">

        </span><span class="n">valuesRate</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">values</span><span class="p">);</span><span class="w"> 

        </span><span class="n">logVal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log2</span><span class="p">(</span><span class="n">valuesRate</span><span class="p">);</span><span class="c1"># log2(0) == infinite</span><span class="w">
        </span><span class="n">logVal</span><span class="p">[</span><span class="nf">is.infinite</span><span class="p">(</span><span class="n">logVal</span><span class="p">)]</span><span class="o">=</span><span class="m">0</span><span class="p">;</span><span class="w">
        
        </span><span class="n">valuesEntropy</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">valuesRate</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">logVal</span><span class="p">;</span><span class="w">
        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nf">is.nan</span><span class="p">(</span><span class="n">valuesEntropy</span><span class="p">)){</span><span class="w">
            </span><span class="n">valuesEntropy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">;</span><span class="w">
        </span><span class="p">}</span><span class="w">
        </span><span class="nf">return</span><span class="p">(</span><span class="n">valuesEntropy</span><span class="p">);</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="n">propNamesAll</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">)</span><span class="w">
    </span><span class="n">propNamesAll</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">propNamesAll</span><span class="p">[</span><span class="nf">length</span><span class="p">(</span><span class="n">propNamesAll</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
    </span><span class="n">print</span><span class="p">(</span><span class="n">propNamesAll</span><span class="p">)</span><span class="w">
    </span><span class="n">buildDecisionTree</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">propNames</span><span class="p">,</span><span class="w"> </span><span class="n">dataSet</span><span class="p">){</span><span class="w">
        
        
        </span><span class="n">classColumn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dataSet</span><span class="p">[,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)]</span><span class="c1">#最后一列是类别标签</span><span class="w">

        </span><span class="n">classSummary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">classColumn</span><span class="p">))</span><span class="c1"># 频次汇总</span><span class="w">

        </span><span class="n">defaultRet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">propNames</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">classSummary</span><span class="p">)[</span><span class="n">which.max</span><span class="p">(</span><span class="n">classSummary</span><span class="p">)]);</span><span class="w">
        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="n">classSummary</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">1</span><span class="p">){</span><span class="c1">#如果所有的都是同一类别，那么标记为叶节点</span><span class="w">
            </span><span class="nf">return</span><span class="p">(</span><span class="n">defaultRet</span><span class="p">);</span><span class="w">
        </span><span class="p">}</span><span class="w">
        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="n">propNames</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">1</span><span class="p">){</span><span class="c1">#如果只剩一种属性了，那么返回样本数量最多的类别作为节点</span><span class="w">
            </span><span class="nf">return</span><span class="p">(</span><span class="n">defaultRet</span><span class="p">);</span><span class="w">
        </span><span class="p">}</span><span class="w">
        </span><span class="n">entropyD</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">calEntropy</span><span class="p">(</span><span class="n">classColumn</span><span class="p">)</span><span class="w">
        </span><span class="n">propGains</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">propNames</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">propName</span><span class="p">){</span><span class="w"> </span><span class="c1"># propName 对应的是"色泽" "根蒂" "敲声" "纹理" "脐部" "触感"</span><span class="w">
            </span><span class="n">propDatas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dataSet</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="n">propName</span><span class="p">)]</span><span class="w">

            </span><span class="n">propClassSummary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">propDatas</span><span class="p">))</span><span class="c1"># 频次汇总</span><span class="w">
            
            </span><span class="n">retGain</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">propClassSummary</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">propClass</span><span class="p">){</span><span class="c1"># propClass 对应色泽的种类 如 浅白 青绿 乌黑</span><span class="w">
                </span><span class="n">dataByPropClass</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span><span class="w"> </span><span class="n">dataSet</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="n">propName</span><span class="p">)]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">propClass</span><span class="p">);</span><span class="w"> </span><span class="c1">#筛选出色泽等于 种类 propClass 的数据集</span><span class="w">
                </span><span class="n">entropyDv</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">calEntropy</span><span class="p">(</span><span class="n">dataByPropClass</span><span class="p">[,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">dataByPropClass</span><span class="p">)])</span><span class="w"> </span><span class="c1">#最后一列是标记是否为好瓜</span><span class="w">
                </span><span class="n">Dv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">propClassSummary</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="n">propClass</span><span class="p">)][</span><span class="m">1</span><span class="p">]</span><span class="w">
                </span><span class="nf">return</span><span class="p">(</span><span class="n">entropyDv</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Dv</span><span class="p">);</span><span class="c1"># 这里没有直接除|D|,最后累加后再除，等价的</span><span class="w">
            </span><span class="p">});</span><span class="w">
            
            </span><span class="nf">return</span><span class="p">(</span><span class="n">entropyD</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">retGain</span><span class="p">)</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">propClassSummary</span><span class="p">));</span><span class="w">
        </span><span class="p">});</span><span class="w">
        </span><span class="c1">#print(propGains);</span><span class="w">
        </span><span class="n">maxEntropyProp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">propGains</span><span class="p">[</span><span class="n">which.max</span><span class="p">(</span><span class="n">propGains</span><span class="p">)];</span><span class="c1">#选择信息熵增益最大的属性</span><span class="w">
        </span><span class="n">propName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">maxEntropyProp</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">
        </span><span class="c1">#print(propName)</span><span class="w">
        </span><span class="n">propDatas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dataSet</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="n">propName</span><span class="p">)]</span><span class="w">

        </span><span class="n">propClassSummary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">propDatas</span><span class="p">))</span><span class="c1"># 频次汇总</span><span class="w">

        </span><span class="n">propClassSummary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">propClassSummary</span><span class="p">[</span><span class="n">which</span><span class="p">(</span><span class="n">propClassSummary</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)]</span><span class="w">
        </span><span class="n">propClassNames</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">propClassSummary</span><span class="p">)</span><span class="w">

        </span><span class="c1">#propClassNames = c(propClassNames[1])</span><span class="w">
        </span><span class="n">retGain</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">propClassNames</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">propClass</span><span class="p">){</span><span class="c1"># propClass 对应色泽的种类 如 浅白 青绿 乌黑</span><span class="w">
            
            </span><span class="n">dataByPropClass</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subset</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span><span class="w"> </span><span class="n">dataSet</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="n">propName</span><span class="p">)]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">propClass</span><span class="p">);</span><span class="w"> </span><span class="c1">#筛选出色泽等于 种类 propClass 的数据集</span><span class="w">
            </span><span class="n">leftClassNames</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">propNames</span><span class="p">[</span><span class="n">which</span><span class="p">(</span><span class="n">propNames</span><span class="o">==</span><span class="n">propName</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">-1</span><span class="p">]</span><span class="w"> </span><span class="c1">#去掉这个属性，递归构造决策树</span><span class="w">
            </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">buildDecisionTree</span><span class="p">(</span><span class="n">leftClassNames</span><span class="p">,</span><span class="w"> </span><span class="n">dataByPropClass</span><span class="p">);</span><span class="w">
            </span><span class="nf">return</span><span class="p">(</span><span class="n">ret</span><span class="p">);</span><span class="w">
        </span><span class="p">});</span><span class="w">
        </span><span class="c1">#names(retGain) = propClassNames</span><span class="w">
        </span><span class="n">retList</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">retGain</span><span class="w">
        </span><span class="c1">#retList = list()</span><span class="w">
        </span><span class="c1">#for (propClass in propClassNames){</span><span class="w">
        </span><span class="c1">#    retList[propClass] = retGain[propClass]</span><span class="w">
        </span><span class="c1">#}</span><span class="w">
        </span><span class="c1">#print(retList)</span><span class="w">

        </span><span class="c1">#索引1表示选择的属性名称 索引2对应的类别，如果有子树那么就是frame，否则就是类别</span><span class="w">
        </span><span class="n">ret</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">propName</span><span class="p">,</span><span class="w"> </span><span class="n">retList</span><span class="p">)</span><span class="w">
        </span><span class="c1">#ret = data.frame(c(retList))</span><span class="w">
        </span><span class="c1">#names(ret) = c(propName)</span><span class="w">
        </span><span class="nf">return</span><span class="p">(</span><span class="n">ret</span><span class="p">);</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="n">retProp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">buildDecisionTree</span><span class="p">(</span><span class="n">propNamesAll</span><span class="p">,</span><span class="w"> </span><span class="n">dataTrain</span><span class="p">);</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">retProp</span><span class="p">);</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">decisionTree</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">trainDecisionTree</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">)</span><span class="w">
</span><span class="c1">#print(decisionTree)</span><span class="w">


</span><span class="n">library</span><span class="p">(</span><span class="s2">"rpart"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"rpart.plot"</span><span class="p">)</span><span class="w">
</span><span class="n">dataTrain</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"xiguadata.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">dataTrain</span><span class="p">)</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rpart</span><span class="p">(</span><span class="n">HaoGua</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">dataTrain</span><span class="p">,</span><span class="n">control</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rpart.control</span><span class="p">(</span><span class="n">minsplit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">minbucket</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="n">method</span><span class="o">=</span><span class="s2">"class"</span><span class="p">)</span><span class="w">
</span><span class="n">printcp</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">

</span><span class="n">rpart.plot</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">branch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">branch.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">extra</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">102</span><span class="p">,</span><span class="n">shadow.col</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span><span class="w"> </span><span class="n">box.col</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">border.col</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="w"> </span><span class="n">split.col</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">main</span><span class="o">=</span><span class="s2">"DecisionTree"</span><span class="p">)</span><span class="w">

</span><span class="c1">#library(jsonlite)</span><span class="w">
</span><span class="c1">#dataJson = toJSON(decisionTree)</span><span class="w">
</span><span class="c1">#c &lt;- file( "result.txt", "w" )</span><span class="w">
</span><span class="c1">#writeLines(dataJson, c )</span><span class="w">
</span><span class="c1">#close( c )   #这里需要主动关闭文件</span><span class="w">

</span><span class="c1">#for (k in propNames) {</span><span class="w">
</span><span class="c1">#    eachData &lt;- dataSet[c(k)]</span><span class="w">
</span><span class="c1">#    values &lt;- table(unlist(eachData))# 频次汇总</span><span class="w">
</span><span class="c1">#    #print(values)</span><span class="w">
</span><span class="c1">#    print(k)</span><span class="w">
</span><span class="c1">#    total &lt;- 0</span><span class="w">
</span><span class="c1">#    for (m in names(values)) {</span><span class="w">
</span><span class="c1">#        #print(m)</span><span class="w">
</span><span class="c1">#        #print(values[m][1])</span><span class="w">
</span><span class="c1">#        data3 &lt;- subset(dataSet, dataSet[c(k)] == m)</span><span class="w">
</span><span class="c1">#        entropyDv &lt;- calEntropy(data3[, length(data3)])</span><span class="w">
</span><span class="c1">#        #print(entropyDv)</span><span class="w">
</span><span class="c1">#        total = total + entropyDv*values[c(m)][1]</span><span class="w">
</span><span class="c1">#    }</span><span class="w">
</span><span class="c1">#    GainDv &lt;- entropyD - total /  sum(values);+</span><span class="w">
</span><span class="c1">#    print(GainDv)</span><span class="w">
</span><span class="c1">#}</span><span class="w">

</span></code></pre></div></div>

<p>R语言代码包含本人自己编写的R语言ID3算法，最后使用R的rpart包训练了一个决策树。</p>

<p><img src="https://img2018.cnblogs.com/blog/282357/201908/282357-20190819151719397-998036118.png" alt=""></p>

<h2 id="总结">总结：</h2>
<ul>
  <li>ID3算法简洁清晰，符合人类思路方式。</li>
  <li>决策树的解释性强，可视化后也方便理解模型和验证正确性。</li>
  <li>ID3算法时候标签类特征的样本，对应具有连续型数值的特征，无法运行此算法。</li>
  <li>有过拟合的风险，要通过剪枝来避免过拟合。</li>
  <li>信息增益有时候偏爱属性很多的特征，C4.5和CART算法可以对此有优化。</li>
  <li>这是我的github主页https://github.com/fanchy，有些有意思的分享。</li>
  <li>python相比R语言写起来还是溜多了，主要是遍历和嵌套，python比R要容易很多，R的数据筛选和选择方便一点，这个python版本的id3算法写的还是很清晰简洁的 正是<em>Talk</em> <em>is</em> <em>cheap</em>. <em>Show</em> <em>me</em> <em>the</em> <em>code</em>。这是在网上可以看到原生实现版本中，最精简的版本之一。</li>
</ul>

<h2 id="对应的西瓜书数据集为">对应的西瓜书数据集为</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>色泽	根蒂	敲声	纹理	脐部	触感	HaoGua
青绿	蜷缩	浊响	清晰	凹陷	硬滑	是
乌黑	蜷缩	沉闷	清晰	凹陷	硬滑	是
乌黑	蜷缩	浊响	清晰	凹陷	硬滑	是
青绿	蜷缩	沉闷	清晰	凹陷	硬滑	是
浅白	蜷缩	浊响	清晰	凹陷	硬滑	是
青绿	稍蜷	浊响	清晰	稍凹	软粘	是
乌黑	稍蜷	浊响	稍糊	稍凹	软粘	是
乌黑	稍蜷	浊响	清晰	稍凹	硬滑	是
乌黑	稍蜷	沉闷	稍糊	稍凹	硬滑	否
青绿	硬挺	清脆	清晰	平坦	软粘	否
浅白	硬挺	清脆	模糊	平坦	硬滑	否
浅白	蜷缩	浊响	模糊	平坦	软粘	否
青绿	稍蜷	浊响	稍糊	凹陷	硬滑	否
浅白	稍蜷	沉闷	稍糊	凹陷	硬滑	否
乌黑	稍蜷	浊响	清晰	稍凹	软粘	否
浅白	蜷缩	浊响	模糊	平坦	硬滑	否
青绿	蜷缩	沉闷	稍糊	稍凹	硬滑	否
</code></pre></div></div>

  </div>

  
</article>


<div id="comments"></div>


<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#comments',
        app_id: '6jSO4XFbk0VGVYP8J0edX7gx-gzGzoHsz',   //这里变量的取值在网站配置文件里_config.yml
        app_key: 'mVHg9pxj49paNoLhs6AFADcy', //这里变量的取值在网站配置文件里_config.yml
        notify:false, 
        verify:false, 
        avatar:'mp', 
        placeholder:'分享你的观点'    //这里变量的取值在网站配置文件里_config.yml
    });
</script>





            
            <footer class="site-footer">
                <!-- SVG icons from https://iconmonstr.com -->
                
                <!-- Github icon -->
                <span class="my-span-icon">
                    <a href="https://github.com/fanchy" aria-label="fanchy's GitHub" title="fanchy's GitHub" target="_blank" rel="noopener noreferrer nofollow">
                        <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>
                    </a>
                </span>
                
                <!-- Twitter icon -->
                <span class="my-span-icon">
                    <a href="https://twitter.com/evanown" aria-label="fanchy's Twitter" title="fanchy's Twitter" target="_blank" rel="noopener noreferrer nofollow">
                        <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24"><path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z"></path></svg>
                    </a>
                </span>
                
                <!-- RSS icon -->
                
                
                <!-- Contact icon -->
                
                
                <span class="my-span-icon">
                    <a href="https://h2cloud.org/contact" aria-label="Contact" title="Contact fanchy">
                        <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24"><path d="M12 .02c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.99 6.98l-6.99 5.666-6.991-5.666h13.981zm.01 10h-14v-8.505l7 5.673 7-5.672v8.504z"></path></svg>
                    </a>
                </span>
                
                <!-- <span class="my-span-icon" style="padding-bottom:5px;"> -->
                    <!-- <a href="https://h2cloud.org/contact" aria-label="Contact" title="Contact fanchy"> -->
                    <!-- 沪ICP备17021230号-1 -->
                    <!-- </a> -->
                <!-- </span> -->
            </footer>
        </section>
        
        <script>
            var menu = document.querySelector("nav.site-nav");
            var checkbox = document.getElementById("nav-trigger");
            
            // close menu if click outside menu
            document.addEventListener("click", function(e) {
                if (menu != e.target &&
                        !isDescendant(menu, e.target)) {
                    checkbox.checked = false;
                }
            }, false);
            
            function isDescendant(parent, child) {
                var node = child.parentNode;
                while (node != null) {
                    if (node == parent) {
                        return true;
                    }
                    node = node.parentNode;
                }
                return false;
            }  
        </script>
        
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0b0b03911bf3e050f9177fccd1e24775";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-124179560-1', 'auto');
  ga('send', 'pageview');
</script>
    </body>
    </html>
    